---
title: 初学爬虫
date: 2016-05-13 17:40:00
tags:
- 爬虫

---

### 工作原理
爬虫，就是伪造一个用户身份，通过URL去访问某个网页，将这个网页上的某些感兴趣的内容，利用正则式的方式提取出来。对于google，baidu的搜索部门，爬虫可以说是核心技术之一。

### python spider
python为实现基本的爬虫，提供了十分方便的模块。通常利用urllib来实现简单的爬虫。首先确定访问的url，然后对这个url生成一个request。request并不仅仅是一个简单的访问请求，它可以采用post，get等方式，来实现数据的提交。

	request = urllib2.Request(url, data, headers)
	response = urllib2.urlopen(request)
	print response.read()
	
post和get的一个差别在于，get的方式是直接以链接形式访问，链接中包含了所有参数。这里简单写一下爬虫中，post和get的使用。

	values = {"username": "user", "password": "pass"}
	#post
	data = urlencode(values)
	request = urllib2.Request(url, data)
	#get
	url = url + "?" + data
	request = urllib2.Request(url)

某些情况下，需要设置访问的headers，才能对网页进行正常的访问。例如防盗链，服务器有时候会检查headers的referer是不是自己。headers是放在request之中的。除了在初始化的时候赋值，还可以添加headers。

	＃设置headers
	headers = { 'User-Agent' : 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)','Referer':'http://www.zhihu.com/articles' }  
	request = urllib2.Request(url, data, headers)
	request.add_header('cache-control', 'no-cache')
	
### 正则表达式
正则表达式是用来提取网页中感兴趣部分的工具。当然，首先自己要对网页源代码有所了解才行。这里列出一个python正则式的匹配规则.  
<img src = "https://github.com/lbxl2345/blogbackup/blob/master/source/pics/%E7%88%AC%E8%99%AB/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F.png?raw=true">  
python中自带了re模块，提供了对于正则表达式的支持。其中，最重要的是re.compile方法。
	
	pattern = re.compile('string');

compile提供一个匹配模式pattern。re中提供了其他的方法，可以根据pattern，在目标中进行匹配/操作。  
	
	re.match(pattern, string[, flags])
	re.search(pattern, string[, flags])
	re.split(pattern, string[, maxsplit])
	re.findall(pattern, string[, flags])
	re.finditer(pattern, string[, flags])
	re.sub(pattern, repl, string[, count])
	re.subn(pattern, repl, string[, count])
	
### 异常处理
爬虫主要会遇到两种Error，URLError和HTTPError。利用try-except来捕捉相应的异常。
	
	#URLError
	try:
		urllib2.urlopen(request)
	except urllib2.URLError, e:
		print e.reason

HTTPError会对应一些状态码，例如404，400等。HTTPError实例化后会有一个code属性，也就是这个相关的错误号。
		
	#HTTPError
	try:
		urllib2.urlopen(rep)
	except urllib2.HTTPError, e:
		print e.code
		print e.reason

如果捕获到了HTTPError，还可以加入hasattr，提前对属性进行判断：  

	except urllib2.URLError, e:
    if hasattr(e,"code"):
        print e.code
    if hasattr(e,"reason"):
        print e.reason
        
### 使用cookie
某些网站为了辨别用户身份，进行session跟踪，需要储存cookie在用户本地终端上。这样一来，某些网站需要登录后才能访问某个页面，此时就需要利用Urllib2库保存我们登陆的Cookie，然后再抓取其他页面。  
获取一个URL时，需要使用一个opener。之前使用的urlopen，实际上是opener的一个特殊实例。对于cookie，python也提供了支持的模块，cookielib。使用cookie分为两个部分，首先是把cookie保存到文件，第二是从文件中获取cookie并访问。cookie保存的过程如下：  

	#创建MozillaCookieJar实例对象，保存cookie
	cookie = cookielib.MozillaCookieJar(filename)
	#利用HTTPCookieProcessor创建一个cookie处理器
	handler = urllib2.HTTPCookieProcessor(cookie)
	req = urllib2.Request("http://www.baidu.com")
	#利用urllib2的buile_opener创建一个opener
	opener = urllib2.build_opener(handler)
	response = opener.open(req)
	#保存cookie到文件
	cookie.save()
	
从文件中获取Cookie并访问的过程如下：

	cookie = cookielib.MozillaCookieJar()
	#读取cookie到变量
	cookie.load('cookie.txt')
	req = urllib2.Request("http://www.baidu.com")
	handler = urllib2.HTTPCookieProcessor(cookie)
	opener =  urllib2.build_opener(handler)
	response = opener.open(req)
	

### 小例子
做了一个小爬虫，用来统计豆瓣电影TOP250名中，前166名的评分总和。这里运用了一些python中的相关知识。  

import urllib
import urllib2
import re
class DBPC:

    def __init__(self,baseUrl):
        self.baseURL = baseURL

    def getPage(self,pageNum):
    #python中全局变量的声明
        global count
        global sum
        try:
            url = self.baseURL + '?start=' + str(pageNum * 25) + '&filter='
            request = urllib2.Request(url)
            response = urllib2.urlopen(request)
            #指定编码格式，并获取内容
            content = response.read().decode('utf-8')
            #获取规则
            pattern = re.compile('<span class="rating_num" property="v:average">(\d\.\d)</span>')
            #利用findall匹配所有
            items = re.findall(pattern, content)
            #通过循环逐个得出
            for item in items:
                print count ,item
                count += 1
                if count > 167:
                    break
                else:
                    sum += float(item)
        except urllib2.URLError, e:
             if hasattr(e,"reason"):
                print e.reason
                return None

	count = 1
	sum = 0
	baseURL = 'https://movie.douban.com/top250'
	dbpc = DBPC(baseURL)
	for i in range(7):
    	dbpc.getPage(i)
	print sum
